#!/usr/bin/env python3
"""
File Organizer - ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ãƒ»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚·ã‚¹ãƒ†ãƒ 
AI Melody Kobo - æŠ•ç¨¿ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¨é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†ä¿å­˜
"""

import os
import sys
import shutil
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import hashlib
import re

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FileOrganizer:
    """ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ãƒ»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, base_dir: str = None):
        """
        åˆæœŸåŒ–
        
        Args:
            base_dir: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.base_dir = Path(base_dir or os.path.dirname(os.path.abspath(__file__)))
        self.archives_dir = self.base_dir / "archives"
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
        self.archive_structure = {
            'articles': self.archives_dir / "articles",
            'images': self.archives_dir / "images", 
            'logs': self.archives_dir / "logs",
            'backup': self.archives_dir / "backup",
            'generated': self.archives_dir / "generated",
            'published': self.archives_dir / "published"
        }
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        self._create_directories()
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«
        self.index_file = self.archives_dir / "file_index.json"
        self.index_data = self._load_index()
    
    def _create_directories(self):
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ"""
        for dir_path in self.archive_structure.values():
            dir_path.mkdir(parents=True, exist_ok=True)
            
        # æ—¥ä»˜åˆ¥ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        today = datetime.now().strftime("%Y/%m")
        for main_dir in ['articles', 'images', 'generated', 'published']:
            monthly_dir = self.archive_structure[main_dir] / today
            monthly_dir.mkdir(parents=True, exist_ok=True)
    
    def _load_index(self) -> Dict:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if self.index_file.exists():
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        return {
            'files': [],
            'articles': [],
            'images': [],
            'last_updated': None,
            'total_files': 0,
            'statistics': {
                'articles_count': 0,
                'images_count': 0,
                'total_size_mb': 0
            }
        }
    
    def _save_index(self):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        self.index_data['last_updated'] = datetime.now().isoformat()
        self.index_data['total_files'] = len(self.index_data['files'])
        
        try:
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(self.index_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def archive_generated_article(self, 
                                 article_content: str,
                                 metadata: Dict,
                                 title: str = None) -> str:
        """
        ç”Ÿæˆã•ã‚ŒãŸè¨˜äº‹ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        
        Args:
            article_content: è¨˜äº‹ã®Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            metadata: è¨˜äº‹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            title: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè‡ªå‹•æŠ½å‡ºã‚‚å¯èƒ½ï¼‰
            
        Returns:
            ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        if not title:
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
            title = self._extract_title_from_content(article_content)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_title = self._sanitize_filename(title)
        filename = f"{timestamp}_{safe_title}.md"
        
        # æœˆåˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
        month_dir = self.archive_structure['generated'] / datetime.now().strftime("%Y/%m")
        month_dir.mkdir(parents=True, exist_ok=True)
        file_path = month_dir / filename
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ 
        full_content = self._add_metadata_header(article_content, metadata, title)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
        file_info = {
            'id': hashlib.md5(str(file_path).encode()).hexdigest()[:8],
            'type': 'generated_article',
            'path': str(file_path.relative_to(self.base_dir)),
            'title': title,
            'filename': filename,
            'created_at': datetime.now().isoformat(),
            'size_bytes': file_path.stat().st_size,
            'metadata': metadata,
            'tags': metadata.get('tags', []),
            'article_type': metadata.get('article_type', 'unknown'),
            'tool_name': metadata.get('tool_name'),
            'status': 'generated'
        }
        
        self.index_data['files'].append(file_info)
        self.index_data['articles'].append(file_info)
        self.index_data['statistics']['articles_count'] += 1
        self._save_index()
        
        logger.info(f"è¨˜äº‹ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã—ãŸ: {file_path}")
        return str(file_path)
    
    def archive_published_article(self,
                                 post_id: int,
                                 post_url: str,
                                 title: str,
                                 content: str,
                                 metadata: Dict) -> str:
        """
        æŠ•ç¨¿æ¸ˆã¿è¨˜äº‹ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        
        Args:
            post_id: WordPressã®æŠ•ç¨¿ID
            post_url: æŠ•ç¨¿URL
            title: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            content: è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_title = self._sanitize_filename(title)
        filename = f"published_{post_id}_{timestamp}_{safe_title}.md"
        
        # æœˆåˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
        month_dir = self.archive_structure['published'] / datetime.now().strftime("%Y/%m")
        month_dir.mkdir(parents=True, exist_ok=True)
        file_path = month_dir / filename
        
        # æŠ•ç¨¿æƒ…å ±ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ 
        publish_info = {
            'post_id': post_id,
            'post_url': post_url,
            'published_at': datetime.now().isoformat(),
            **metadata
        }
        
        full_content = self._add_metadata_header(content, publish_info, title)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
        file_info = {
            'id': hashlib.md5(str(file_path).encode()).hexdigest()[:8],
            'type': 'published_article',
            'path': str(file_path.relative_to(self.base_dir)),
            'title': title,
            'filename': filename,
            'created_at': datetime.now().isoformat(),
            'size_bytes': file_path.stat().st_size,
            'post_id': post_id,
            'post_url': post_url,
            'metadata': publish_info,
            'status': 'published'
        }
        
        self.index_data['files'].append(file_info)
        self.index_data['articles'].append(file_info)
        self.index_data['statistics']['articles_count'] += 1
        self._save_index()
        
        logger.info(f"æŠ•ç¨¿æ¸ˆã¿è¨˜äº‹ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã—ãŸ: {file_path}")
        return str(file_path)
    
    def archive_image(self, 
                     image_data: bytes,
                     filename: str,
                     metadata: Dict = None) -> str:
        """
        ç”»åƒã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        
        Args:
            image_data: ç”»åƒã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        # æœˆåˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
        month_dir = self.archive_structure['images'] / datetime.now().strftime("%Y/%m")
        month_dir.mkdir(parents=True, exist_ok=True)
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¿½åŠ 
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        name, ext = os.path.splitext(filename)
        timestamped_filename = f"{timestamp}_{name}{ext}"
        file_path = month_dir / timestamped_filename
        
        # ç”»åƒä¿å­˜
        with open(file_path, 'wb') as f:
            f.write(image_data)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜
        if metadata:
            metadata_path = file_path.with_suffix('.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
        file_info = {
            'id': hashlib.md5(str(file_path).encode()).hexdigest()[:8],
            'type': 'image',
            'path': str(file_path.relative_to(self.base_dir)),
            'filename': timestamped_filename,
            'original_filename': filename,
            'created_at': datetime.now().isoformat(),
            'size_bytes': file_path.stat().st_size,
            'metadata': metadata or {},
            'has_metadata_file': bool(metadata)
        }
        
        self.index_data['files'].append(file_info)
        self.index_data['images'].append(file_info)
        self.index_data['statistics']['images_count'] += 1
        self._save_index()
        
        logger.info(f"ç”»åƒã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã—ãŸ: {file_path}")
        return str(file_path)
    
    def organize_old_files(self):
        """æ—¢å­˜ã®æ•£ã‚‰ã°ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´ç†"""
        logger.info("æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´ç†ã‚’é–‹å§‹...")
        
        # æ•´ç†å¯¾è±¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = {
            'articles': ['*.md', 'final_article_*.md', 'sample_article_*.md'],
            'images': ['*.png', '*.jpg', '*.jpeg'],
            'logs': ['*.log'],
            'generated': ['generate_*.py', '*_article.py']
        }
        
        organized_count = 0
        
        for category, file_patterns in patterns.items():
            for pattern in file_patterns:
                for file_path in self.base_dir.glob(pattern):
                    if file_path.is_file() and not file_path.name.startswith('.'):
                        self._move_to_archive(file_path, category)
                        organized_count += 1
        
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†å®Œäº†: {organized_count}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç§»å‹•")
    
    def _move_to_archive(self, file_path: Path, category: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•"""
        target_dir = self.archive_structure[category]
        
        # åŒåãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¿½åŠ 
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if (target_dir / file_path.name).exists():
            name, ext = os.path.splitext(file_path.name)
            new_name = f"{name}_{timestamp}{ext}"
        else:
            new_name = file_path.name
        
        target_path = target_dir / new_name
        shutil.move(str(file_path), str(target_path))
        logger.info(f"ç§»å‹•: {file_path.name} -> {target_path}")
    
    def _extract_title_from_content(self, content: str) -> str:
        """Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('# '):
                return line[2:].strip()
            elif line.startswith('## ') and not line.startswith('### '):
                return line[3:].strip()
        
        # ã‚¿ã‚¤ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        return f"è¨˜äº‹_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    def _sanitize_filename(self, filename: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å®‰å…¨ãªå½¢å¼ã«å¤‰æ›"""
        # å±é™ºãªæ–‡å­—ã‚’é™¤å»
        safe_chars = re.sub(r'[<>:"/\\|?*]', '', filename)
        # é•·ã•åˆ¶é™
        if len(safe_chars) > 50:
            safe_chars = safe_chars[:50]
        return safe_chars or "unnamed"
    
    def _add_metadata_header(self, content: str, metadata: Dict, title: str) -> str:
        """è¨˜äº‹ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ """
        header = "---\n"
        header += f"title: {title}\n"
        header += f"archived_at: {datetime.now().isoformat()}\n"
        
        for key, value in metadata.items():
            if key not in ['title']:
                header += f"{key}: {value}\n"
        
        header += "---\n\n"
        return header + content
    
    def get_statistics(self) -> Dict:
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        total_size = sum(
            Path(self.base_dir / file_info['path']).stat().st_size 
            for file_info in self.index_data['files']
            if Path(self.base_dir / file_info['path']).exists()
        )
        
        return {
            'total_files': len(self.index_data['files']),
            'articles_count': len(self.index_data['articles']),
            'images_count': len(self.index_data['images']),
            'total_size_mb': round(total_size / (1024 * 1024), 2),
            'last_updated': self.index_data['last_updated'],
            'archive_directories': list(self.archive_structure.keys())
        }
    
    def search_files(self, query: str, file_type: str = None) -> List[Dict]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        results = []
        query_lower = query.lower()
        
        for file_info in self.index_data['files']:
            if file_type and file_info['type'] != file_type:
                continue
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã€ãƒ•ã‚¡ã‚¤ãƒ«åã€ã‚¿ã‚°ã§æ¤œç´¢
            if (query_lower in file_info.get('title', '').lower() or
                query_lower in file_info['filename'].lower() or
                any(query_lower in tag.lower() for tag in file_info.get('tags', []))):
                results.append(file_info)
        
        return results
    
    def list_recent_files(self, limit: int = 10, file_type: str = None) -> List[Dict]:
        """æœ€è¿‘ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        filtered_files = self.index_data['files']
        
        if file_type:
            filtered_files = [f for f in filtered_files if f['type'] == file_type]
        
        # ä½œæˆæ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆ
        sorted_files = sorted(
            filtered_files,
            key=lambda x: x['created_at'],
            reverse=True
        )
        
        return sorted_files[:limit]


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='AI Melody Kobo - ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ãƒ»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚·ã‚¹ãƒ†ãƒ '
    )
    parser.add_argument(
        '--organize',
        action='store_true',
        help='æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´ç†'
    )
    parser.add_argument(
        '--stats',
        action='store_true',
        help='çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º'
    )
    parser.add_argument(
        '--search',
        help='ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢'
    )
    parser.add_argument(
        '--recent',
        type=int,
        default=10,
        help='æœ€è¿‘ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤ºï¼ˆä»¶æ•°æŒ‡å®šï¼‰'
    )
    parser.add_argument(
        '--type',
        choices=['generated_article', 'published_article', 'image'],
        help='ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼'
    )
    
    args = parser.parse_args()
    
    organizer = FileOrganizer()
    
    if args.organize:
        organizer.organize_old_files()
    
    if args.stats:
        stats = organizer.get_statistics()
        print("\nğŸ“Š ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–çµ±è¨ˆ:")
        print(f"   ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['total_files']}")
        print(f"   è¨˜äº‹æ•°: {stats['articles_count']}")
        print(f"   ç”»åƒæ•°: {stats['images_count']}")
        print(f"   ç·ã‚µã‚¤ã‚º: {stats['total_size_mb']} MB")
        print(f"   æœ€çµ‚æ›´æ–°: {stats['last_updated']}")
    
    if args.search:
        results = organizer.search_files(args.search, args.type)
        print(f"\nğŸ” æ¤œç´¢çµæœ: '{args.search}'")
        for file_info in results[:10]:
            print(f"   {file_info['filename']} ({file_info['type']})")
    
    if not any([args.organize, args.stats, args.search]):
        recent_files = organizer.list_recent_files(args.recent, args.type)
        print(f"\nğŸ“ æœ€è¿‘ã®ãƒ•ã‚¡ã‚¤ãƒ« ({len(recent_files)}ä»¶):")
        for file_info in recent_files:
            print(f"   {file_info['created_at'][:16]} - {file_info['filename']}")


if __name__ == "__main__":
    main()